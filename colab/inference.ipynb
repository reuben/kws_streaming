{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference.ipynb",
      "provenance": [
        {
          "file_id": "1MOVSh4xDo6S0K37DK-UFDt-_5ZQtywgA",
          "timestamp": 1579833932754
        },
        {
          "file_id": "kws_streaming/colab/inference.ipynb?cl=288814320",
          "timestamp": 1579717628043
        },
        {
          "file_id": "1WABxJM9EOzIcuVwgSEc8UzwLiDsjdOD9",
          "timestamp": 1566852779477
        }
      ],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXYgXFeMgRep",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y55h79H3XKSt"
      },
      "source": [
        "# Examples of streaming and non streaming inference with TF/TFlite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KyEW-pDlnzuS"
      },
      "source": [
        "## Prepare colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MpV3uzPn8UM",
        "colab": {}
      },
      "source": [
        "# bazel build -c opt --copt=-mavx2 //kws_streaming/colab:colab_notebook.par\n",
        "# ./bazel-bin/kws_streaming/colab/colab_notebook.par"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fathHzuEgx8_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yP5WBy5O8Za8",
        "colab": {}
      },
      "source": [
        "# TF streaming\n",
        "from kws_streaming.models import models\n",
        "from kws_streaming.models import utils\n",
        "from kws_streaming.layers.modes import Modes\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zMdTK10tL2Dz",
        "colab": {}
      },
      "source": [
        "# general imports\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from colabtools import sound\n",
        "import numpy as np\n",
        "import scipy as scipy\n",
        "import scipy.io.wavfile as wav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ylPGCTPLh41F"
      },
      "source": [
        "## Load wav file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkHD1EFQIL2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.signal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8Bvq7XacsOu",
        "colab": {}
      },
      "source": [
        "def waveread_as_pcm16(filename):\n",
        "  \"\"\"Read in audio data from a wav file.  Return d, sr.\"\"\"\n",
        "  file_handle = tf.io.gfile.GFile(filename, 'rb')\n",
        "  try:\n",
        "    samplerate, wave_data = wav.read(file_handle)\n",
        "  finally:\n",
        "    file_handle.close()  \n",
        "  # Read in wav file.  \n",
        "  return wave_data, samplerate  \n",
        "\n",
        "def wavread_as_float(filename, target_sample_rate=16000):\n",
        "  \"\"\"Read in audio data from a wav file.  Return d, sr.\"\"\"\n",
        "  wave_data, samplerate = waveread_as_pcm16(filename)\n",
        "  desired_length = int(\n",
        "          round(float(len(wave_data)) / samplerate * target_sample_rate))\n",
        "  wave_data = scipy.signal.resample(wave_data, desired_length)\n",
        "\n",
        "  # Normalize short ints to floats in range [-1..1).\n",
        "  data = np.array(wave_data, np.float32) / 32768.0\n",
        "  return data, target_sample_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TYj0JGeHhtqc",
        "colab": {}
      },
      "source": [
        "# Set path to wav file for testing.\n",
        "wav_file = \"../data2/left/00970ce1_nohash_0.wav \"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U7VYKfWoh_3-",
        "colab": {}
      },
      "source": [
        "wav_data, samplerate = wavread_as_float(wav_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jNiuJTvXiF1J",
        "colab": {}
      },
      "source": [
        "assert samplerate == 16000\n",
        "sound.Play(wav_data, samplerate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r2yeKkLsiRWJ",
        "colab": {}
      },
      "source": [
        "plt.plot(wav_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qPifrGCTL0re",
        "colab": {}
      },
      "source": [
        "input_data = np.expand_dims(wav_data, 0)\n",
        "input_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5_wbAZ3vhQh1"
      },
      "source": [
        "## Prepare batched model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y6FTCF0j8x3h",
        "colab": {}
      },
      "source": [
        "# Set path to model weights and model parameters\n",
        "train_dir = \"../data2/models/svdf/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2f-1Ioqbn4G",
        "colab": {}
      },
      "source": [
        "load command line command flags which were use for model creation/training\n",
        "from argparse import Namespace\n",
        "with tf.io.gfile.Open(os.path.join(train_dir, 'flags.txt'), 'rt') as fd:\n",
        "  flags_txt = fd.read()\n",
        "flags = eval(flags_txt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzf9TrLRmkXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with tf.io.gfile.Open(os.path.join(train_dir, 'flags.json'), 'rt') as fd:\n",
        "#   flags_json = json.load(fd)\n",
        "\n",
        "# class DictStruct(object):\n",
        "#   def __init__(self, **entries):\n",
        "#     self.__dict__.update(entries)\n",
        "\n",
        "# flags = DictStruct(**flags_json)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wsGDG4A0cIMO",
        "colab": {}
      },
      "source": [
        "# create model with flag's parameters \n",
        "model_non_stream_batch = models.MODELS[flags.model_name](flags)\n",
        "\n",
        "# load model's weights\n",
        "weights_name = 'best_weights'\n",
        "model_non_stream_batch.load_weights(os.path.join(train_dir, weights_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsWLekwbkdTo",
        "colab": {}
      },
      "source": [
        "#model_non_stream_batch.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QVhESthmMl0X",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model_non_stream_batch,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    expand_nested=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiamrs3Ljx2g"
      },
      "source": [
        "## Read labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvigTFxRjk6k",
        "colab": {}
      },
      "source": [
        "with tf.io.gfile.Open(os.path.join(train_dir, 'labels.txt'), 'rt') as fd:\n",
        "  labels_txt = fd.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DANGBMV0jDme",
        "colab": {}
      },
      "source": [
        "labels = labels_txt.split()\n",
        "labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RIr1DWLisMu9"
      },
      "source": [
        "## Run inference with TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "456ynjRxmdVc"
      },
      "source": [
        "### TF Run non streaming inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-vJpOCJClDK5",
        "colab": {}
      },
      "source": [
        "# convert model to inference mode with batch one\n",
        "inference_batch_size = 1\n",
        "tf.keras.backend.set_learning_phase(0)\n",
        "flags.batch_size = inference_batch_size  # set batch size\n",
        "\n",
        "model_non_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.NON_STREAM_INFERENCE)\n",
        "#model_non_stream.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1gOGQjWMufh",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model_non_stream,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    expand_nested=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nPUfT4a4lxIj",
        "colab": {}
      },
      "source": [
        "predictions = model_non_stream.predict(input_data)\n",
        "predicted_labels = np.argmax(predictions, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "63sisD1hl7jz",
        "colab": {}
      },
      "source": [
        "predicted_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rBhLA1OZmQxj",
        "colab": {}
      },
      "source": [
        "labels[predicted_labels[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZVFoVdYSpnL_"
      },
      "source": [
        "### TF Run streaming inference with internal state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cgcpcrASquAY",
        "colab": {}
      },
      "source": [
        "# convert model to streaming mode\n",
        "flags.batch_size = inference_batch_size  # set batch size\n",
        "\n",
        "model_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.STREAM_INTERNAL_STATE_INFERENCE)\n",
        "#model_stream.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BNtgTOBCM06v",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model_stream,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    expand_nested=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7NOG8wrYpnnq",
        "colab": {}
      },
      "source": [
        "# run streaming inference\n",
        "start = 0\n",
        "end = flags.window_stride_samples\n",
        "while end <= input_data.shape[1]:\n",
        "  stream_update = input_data[:, start:end]\n",
        "\n",
        "  # get new frame from stream of data\n",
        "  stream_output_prediction = model_stream.predict(stream_update)\n",
        "  stream_output_arg = np.argmax(stream_output_prediction)\n",
        "\n",
        "  # update indexes of streamed updates\n",
        "  start = end\n",
        "  end = start + flags.window_stride_samples\n",
        "\n",
        "stream_output_arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-xeXPhAqC20",
        "colab": {}
      },
      "source": [
        "labels[stream_output_arg]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F5WYgOtSqrQb"
      },
      "source": [
        "### TF Run streaming inference with external state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2hTLEY1qq_ig",
        "colab": {}
      },
      "source": [
        "# convert model to streaming mode\n",
        "flags.batch_size = inference_batch_size  # set batch size\n",
        "\n",
        "model_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.STREAM_EXTERNAL_STATE_INFERENCE)\n",
        "#model_stream.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AyeABeg9Mbf6",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model_stream,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    expand_nested=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RISdLTnmqrcA",
        "colab": {}
      },
      "source": [
        "\n",
        "inputs = []\n",
        "for s in range(len(model_stream.inputs)):\n",
        "  inputs.append(np.zeros(model_stream.inputs[s].shape, dtype=np.float32))\n",
        "\n",
        "reset_state = True\n",
        "\n",
        "if reset_state:\n",
        "  for s in range(len(model_stream.inputs)):\n",
        "    inputs[s] = np.zeros(model_stream.inputs[s].shape, dtype=np.float32)\n",
        "\n",
        "start = 0\n",
        "end = flags.window_stride_samples\n",
        "while end <= input_data.shape[1]:\n",
        "  # get new frame from stream of data\n",
        "  stream_update = input_data[:, start:end]\n",
        "\n",
        "  # update indexes of streamed updates\n",
        "  start = end\n",
        "  end = start + flags.window_stride_samples\n",
        "\n",
        "  # set input audio data (by default input data at index 0)\n",
        "  inputs[0] = stream_update\n",
        "\n",
        "  # run inference\n",
        "  outputs = model_stream.predict(inputs)\n",
        "\n",
        "  # get output states and set it back to input states\n",
        "  # which will be fed in the next inference cycle\n",
        "  for s in range(1, len(model_stream.inputs)):\n",
        "    inputs[s] = outputs[s]\n",
        "\n",
        "  stream_output_arg = np.argmax(outputs[0])\n",
        "stream_output_arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u6p1xubwrYyo",
        "colab": {}
      },
      "source": [
        "labels[stream_output_arg]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TAWcyUKDvYqm"
      },
      "source": [
        "##Run inference with TFlite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KAJs5dBXsYCa"
      },
      "source": [
        "### Run non streaming inference with TFLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "88bclN4rtu-5",
        "colab": {}
      },
      "source": [
        "path = os.path.join(train_dir, 'tflite_non_stream')\n",
        "tflite_model_name = 'non_stream.tflite'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VZgH11_0u2ZN",
        "colab": {}
      },
      "source": [
        "# prepare TFLite interpreter\n",
        "with tf.io.gfile.Open(os.path.join(path, tflite_model_name), 'rb') as f:\n",
        "  model_content = f.read()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=model_content)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "inputs = []\n",
        "for s in range(len(input_details)):\n",
        "  inputs.append(np.zeros(input_details[s]['shape'], dtype=np.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J2n7VB5JxV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded_input = np.zeros((1, 16000), dtype=np.float32)\n",
        "padded_input[:, :input_data.shape[1]] = input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TXqHxLcVregL",
        "colab": {}
      },
      "source": [
        "# set input audio data (by default input data at index 0)\n",
        "interpreter.set_tensor(input_details[0]['index'], padded_input.astype(np.float32))\n",
        "\n",
        "# run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# get output: classification\n",
        "out_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "out_tflite_argmax = np.argmax(out_tflite)\n",
        "\n",
        "out_tflite_argmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbIB3zaiKEru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(out_tflite)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eOk-7ZBQrtMa",
        "colab": {}
      },
      "source": [
        "labels[out_tflite_argmax]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xNaUWgivuatL"
      },
      "source": [
        "### Run streaming inference with TFLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aAEnF2PI8i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "csQWZo4BuqEB",
        "colab": {}
      },
      "source": [
        "path = os.path.join(train_dir, 'tflite_stream_state_external')\n",
        "tflite_model_name = 'stream_state_external.tflite'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4wAZqYouyob",
        "colab": {}
      },
      "source": [
        "with tf.io.gfile.Open(os.path.join(path, tflite_model_name), 'rb') as f:\n",
        "  model_content = f.read()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=model_content)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "inputs = []\n",
        "for s in range(len(input_details)):\n",
        "  inputs.append(np.zeros(input_details[s]['shape'], dtype=np.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03QCq1nfVUWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_details[0]['shape']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WKudF1Zyud2-",
        "colab": {}
      },
      "source": [
        "reset_state = True\n",
        "\n",
        "# before processing new test sequence we can reset model state\n",
        "# if we reset model state then it is not real streaming mode\n",
        "if reset_state:\n",
        "  for s in range(len(input_details)):\n",
        "    print(input_details[s]['shape'])\n",
        "    inputs[s] = np.zeros(input_details[s]['shape'], dtype=np.float32)\n",
        "\n",
        "start = 0\n",
        "end = flags.window_stride_samples\n",
        "while end <= input_data.shape[1]:\n",
        "  stream_update = input_data[:, start:end]\n",
        "  stream_update = stream_update.astype(np.float32)\n",
        "\n",
        "  # update indexes of streamed updates\n",
        "  start = end\n",
        "  end = start + flags.window_stride_samples\n",
        "\n",
        "  # set input audio data (by default input data at index 0)\n",
        "  interpreter.set_tensor(input_details[0]['index'], stream_update)\n",
        "\n",
        "  # set input states (index 1...)\n",
        "  for s in range(1, len(input_details)):\n",
        "    interpreter.set_tensor(input_details[s]['index'], inputs[s])\n",
        "\n",
        "  # run inference\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # get output: classification\n",
        "  out_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
        "  print(start / 16000.0, np.argmax(out_tflite), np.max(out_tflite))\n",
        "\n",
        "  # get output states and set it back to input states\n",
        "  # which will be fed in the next inference cycle\n",
        "  for s in range(1, len(input_details)):\n",
        "    # The function `get_tensor()` returns a copy of the tensor data.\n",
        "    # Use `tensor()` in order to get a pointer to the tensor.\n",
        "    inputs[s] = interpreter.get_tensor(output_details[s]['index'])\n",
        "\n",
        "  out_tflite_argmax = np.argmax(out_tflite)\n",
        "out_tflite_argmax  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWy_BiepFFSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(out_tflite)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ssfckfpHvOhJ",
        "colab": {}
      },
      "source": [
        "labels[out_tflite_argmax]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QSa7AX1GvReF",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}